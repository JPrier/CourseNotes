\documentclass{article}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\usepackage{listings}
\usepackage{color}
\usepackage{amsmath}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	language=Python,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

\title{CSC263 Notes}
\author{Joshua Prier}
\date{January 10, 2017}

\begin{document}
	\maketitle
	\section{Week One}
	\subsection{Day One: Big-O Review}
	
		Reading: Asymptotic Bounds\\
		\\
		Big-Oh notation is a mathematical notation that classifies functions according to their growth rate\\
		$$O(n^2) = \{n^2, 4n^{\alpha}, n^{\alpha} + n, ... \}$$
		Big-Oh is a set of functions that\textbf{ grow at most as fast} as the given function, i.e $n^2$\\
		$$\Omega(n^2) = \{n^2, 4n^2, n^2 + n, n^3, n^4, ...\}$$
		Big-Omega is a set of functions that \textbf{grow at least as fast} as the given function\\
		$$\Theta(n^2) = \Omega(n^2) \cap O(n^2) = \{n^2, 4n^2, n^2 + n, ...\}$$
		Big-Theta is a set of functions that \textbf{grow as fast} as the given function\\
		\\
		In this course we will restrict our function f() to be the worst-case time complexity of an algorithm\\
		\\
		\begin{center}
			\large{How do we \underline{\textbf{prove}} asymptotic bounds for worst-case time complexity?}\\
		\end{center}
		
		Let t(x) be the number of steps taken by algorithm A on input x\\
		Let T(n) be the worst-case time complexity of algorithm A\\
		T(n) = max t(x) on all inputs x of size n\\
		\\
		\begin{enumerate}
			\item To prove $T(n)\in O(g(n))$ show there is a constant $c>0$ and an input size $n_0 >0$ such that for all $n \ge n_0$\\
			$$T(n) \le cg(n)$$
			$\iff max\{t(x); x$ is an input of size $n \} \le cg(n)$\\
			\\
			$\iff$ for \textbf{\emph{every}} input of size n, A takes \textbf{\emph{at most}} $cg(n)$ steps\\
			\\\\
			\item To prove $T(n)\in\Omega (g(n))$ show there is a constant $c>0$ and an input size $n_0 >0$, such that for all $n\ge n_0$\\
			$$T(n)\ge cg(n)$$\\
			$\iff max\{t(x); x$ is an input of size $n\} \ge cg(n)$\\\\
			$\iff$ for \textbf{\emph{some}} input of size n, A takes \textbf{\emph{at least}} cg(n) steps\\\\
			\\
			\item To prove $T(n)\in\Theta (g(n))$ show there is a constant $c>0$ and an input size $n_0 >0$, such that for all $n \ge n_0$\\
			$$T(n) = cg(n)$$\\
			$\iff max\{t(x); x$ is an input of size $n\} = cg(n)$\\\\
			$\iff$ for \textbf{\emph{every}} input of size n, A takes \textbf{\emph{exactly}} cg(n) steps\\\\\\\\
			
			
		\end{enumerate}
	
	\subsection{Day Two: Abstract Data Types}
		Exercises: All 6.1, 6.2-6, 6.3-2, 6.3-3\\
		Readings: Read code for MAX-HEAPIFY in textbook and write an iterative implementation\\
		\\\\
		Abstract data types describe an object and its operation in the POV of the user\\
		Data Structure is a specific implementation of an ADT, POV of the coder
		\\\\
		PQ Object: sets of elements with "keys" ("priority") that can be compared
		\\\\
		PQ Operations:
		\begin{enumerate}
			\item Insert(S,x): Insert x in S $S<-S\cup$ \{x\}
			\item max(S): Return an element wiht highest priority in S 
			\item Extract\_Max(S): Returns x$<-$max(S) AND removes it from s
		\end{enumerate}
		"Naive Implementations"\\
		Linked List: Insert- O(1), Max- O(N)\\
		Sorted Linked List: Insert- O(N), Max- O(1)\\\\
		Heap data structure\\
		performs both in O(lg n)\\\
		
		Abstract representation of heap: ADD THIS IN\\
		
		Max Heap Property:\\
		The key of the parent node is \textbf{at least} the value of the key's of it's children\\
		
		Min Heap Property:\\
		The key of the parent node is \textbf{at most} the value of the key's of it's children\\
		
		Implementation:\\
		("going down" one level of abstraction from the tree representation)\\
		\begin{tabular}{r|cl}
			content.
		\end{tabular}
		\\
		$Root(A) = 1$\\
		$Parent(i) = \floor{\frac{i}{2}}$\\
		$Left(i) = 2i$\\
		$Right(i) = 2i + 1$\\
		\\
		Max-Heap:\\
		\begin{itemize}
			\item Subprocedure for all other heap operations
			\item Precondition: Left(i) and Right(i) are both MAX HEAPS
			\item Idea: A[i] might be smaller than it's children (i.e. doesn't satisfy the heap property) and thus it needs to "float down"
		\end{itemize}
		Place max heap drawings here\\\\
		To build a Heap you sort an array one object at a time\\\\
		sudo code:\\
		\begin{enumerate}
			\item check if Left(i) and Right(i) exist
			\item L= max\_index(A[i], A[Left(i)], A[Right(i)])
			\item if L$\neq$ i: exchange A[L] and A[i], MAX-HEAPIFY(A,L)\\\\\\
		\end{enumerate}
		Note: Height of binary tree with n nodes is log(n)\\
		
		$$\sum_{i=0}^{h} 2^i = 2^{h+1} -1$$
		
		MAX(A) = A[1]\\
		EXTRACT-MAX(A)
		\begin{itemize}
			\item swap root last element
			\item call heapify on root
			\item decrease array by 1
		\end{itemize}
		
	\subsection{Lab 1: Time Complexity Review}	
		
		TA: Yue Jiang\\
		Email: yuenj.jiang@mail.utoronto.ca\\\\
		\begin{center}
			T(n) = worst case time complexity\\
			t(x) = number of steps taken by algorithm A on input x\\
			T(n) = max{t(x) | x is an inoout of size n}\\
		\end{center}
		 - lower bound is any line under the worst case\\
		 - upper bound is any line over the worst case \\\\\\
		 \begin{lstlisting}
		 SeqSearch(A[1,...,n], x):
			 i = 1
			 found = False
			 while i <= n and not found:
				 if A[i] = x:
					 found = True
				 i = i + 1
			 return found
		 \end{lstlisting}
		 Let T(n) = worst case time complexity of SeqSearch\\
		 $$T(n)=\Theta(n)~iff~T(n)=O(n)~and~T(n)=\Omega(n)$$
		 O: Analyze the loops\\
		 $\Omega$: Give a bad example\\
		 n iterations\\
		 In iteration we take O(1) time\\
		 T(n) = n * O(1) = O(n)\\
		 For fixed n, (insert table 2), when x is in the last entry of A and x is not other entries, we have the \textbf{\emph{worst case}}\\
		 \begin{lstlisting}
		 BubbleSort(A[1,...n]):
			 last = n
			 sorted = False
			 while not sorted:
				 sorted = True
				 for j = 1 to last - 1:
					 if A[j] > A[j + 1]:
						 swap(A[j], A[j + 1])
						 sorted = False
				 last = last - 1
			 return A[1,...,n]
		 \end{lstlisting}
		 \textbf{Sorting Example:}\\
		        ~~~~~~2 3 7 1 4\\
		  1st   2 3 1 7 4\\
		        ~~~~~~~2 3 1 4 7\\
		  2nd   2 1 3 4 7\\
		  3rd   1 2 3 4 7\\\\
		  last = n, last = last - 1, last = 1\\
		  while n - 1 iterations, n(n-1) > 1\\
		  for $\le$ n-1 iterations\\
		  T(n) = (n-1)(n-1)O(n)\\\\
		  \textbf{Worst Case Example:}\\
			The number in the array is in decreasing order\\
			$(n-1)+(n-2)+...+1=\frac{n(n-1)}{2}$ steps\\\\\\\\\\
		\large{\textbf{Heap}}\\
		\textbf{Methods:}\\
		Insert()\\
		ExtractMax()\\
		
	\section{Week Two}
		\subsection{Day One: Binomial Heaps}
		Exercises: 12.2-3 and 12.3-1 \\\\
		Merging regular heaps?\\
		- concat. both heaps\\
		- call Build-Heap (runs in O(n))\\\\
		\textbf{We Want:} To merge in O(log N) time\\\\
		Substructure of Binomial Heap: Binomial Tree\\
		 - recursive def$^{n}$ of \emph{shape}\\
		 (Binomial Trees figure 2.1.1)\\
		 (depth table for B$_4$ figure 2.1.2)\\\\
		 \textbf{Properties of Bin. Trees:} (easily proven with induction, try with exercise)\\
		    For Bin. Tree B$_k$
		 \begin{enumerate}
		 	\item There are $2^k$ nodes
		 	\item Height is $k$
		 	\item At depth $i$ B$_k$ has $_k$C$_i$ nodes
		 	\item B$_k$'s root has a degree k and it's children have shape: B$_{k-1}$, B$_{k-2}$, ..., B$_1$, B$_0$ (fig 2.1.3)\\
		 \end{enumerate}
		 \textbf{\Large Data Structure: Binomial Heap}\\ \large
		 - Sequennce H$_n$ of binomial trees that satisfy the following properties
		 \begin{enumerate}
		 	\item each tree satisfies the heap property
		 	\item The sequence consists of \textbf{\emph{strictly increasing}} $k$'s\\
		 	(fig 2.1.4)\\
		 \end{enumerate}
		 Relation to binary representation: \\
		 n = $<$ b$_{\floor{lg n}}$ ... b$_1$ b$_0$ $>_2$\\
		 n = $\sum_{i=0}^{\floor{lg n}} b_i 2^i$ (binary representation is \textbf{\emph{unique}})\\
		 Ex: n nodes in bin heap (fig 2.1.5)\\
		 Ex: 8 = $< 1000 >_2$ (fig 2.1.6)\\\\
		 Let $\alpha$(n): \# of 1's in bin representation of n\\
		 a) $H_n$ has $\alpha$(n) trees\\
		 b) $H_n$ has n - $\alpha$(n) edges \textbf{(proving this in Assignment \#1)}\\\\
		 Min(H) = iterate over roots\\
		 Time Complexity: O(log n) *Not $\Theta$(log n)*\\\\
		 Note: Length of Bin Rep. of n is $\floor{log n}$\\
		 
		 \subsection{Day Two: }
		 ADT: Mergeable priority queue\\
		 \begin{itemize}
		 	\item MIN(H)
		 	\item INSERT(H, x)
		 	\item EXTRACT\_MIN(x)
		 	\item UNION(H$_1$, H$_2$)\\
		 \end{itemize}
		Data Structure that implements Mergeable priority queue: Binomial Heap\\
		(fig. 2.2.1)\\
		Assume we have a UNION(H$_1$, H$_2$) operation: (fig 2.2.2)\\\\\\\\
		How would you implement INSERT(H, x) and EXTRACT\_MIN(H) using UNION?\\\\
		INSERT(H,x)
		\begin{itemize}
			\item create a one element heap H containing x
			\item call UNION(H$_1$, H$_2$)
		\end{itemize}
		Time Complexity: O(log n)\\\\
		EXTRACT\_MIN(H)
		\begin{itemize}
			\item Find binomial tree in H with min root, call Tree T (O(log n))
			\item H$_1$ = H - T (Constant)
			\item H$_2$ = children of T's root (Constant)
			\item UNION(H$_1$, H$_2$) (O(log n))
		\end{itemize}
		Time Complexity: O(log n)\\\\
		\textbf{Example: } UNION(H$_1$, H$_2$)\\
		(fig 2.2.3)\\
		\textbf{Note}: It is not a Binomial Heap if there is multiple of the same length trees\\
		\textbf{Note}: the actual process of the merge is linear\\
		\emph{Alg:} Loop over all trees from smallest to largest, if there are 2 or 3 B$_i$'s in the collection, take the first 2 and link one tree to the root of the other\\
		(fig 2.2.4)\\
		(fig 2.2.5)\\\\
		Runtime of UNION(H$_1$, H$_2$)\\
		Merge: O(log n) where n = $\|H_1\| + \|H_2\|$\\
		Max number of roots is $\floor{lg n} + 1$\\
		\emph{How many links?}\\
		For every size k, at most 1 link operation\\
		why at most 1?
		\begin{itemize}
			\item because each binomial tree has at most 1 tree of size k and there is at most 1 carry
		\end{itemize}
		if\\ 
		1: leave it\\
		2: link them\\
		3: link 2 of them\\
		Time Complexity: O(lg n) + O(lg n), (Constant time "linking" for each k)\\\\
		
		\subsection{Lab 2: Abstract Data Types}
		\textbf{Abstract Data Type (ADT):} What you want to do\\
		e.g Priority Queue: First in first out\\\\
		\textbf{Data Structure:} How to implement\\
		e.g Heap:\\
		Precondition: the subtree rooted at left(i) and right(i) are heaps\\
		Postcondition: The subtree rooted at i is a heap
		\begin{lstlisting}
			MaxHeapify(A, i):
				largest = index of max(A[i], A[left(i)], A[right(i)])
				if largest != i:
					then swap(A[i], A[largest])
					MaxHeapify(A, largest)
					
			BuildMaxHeap(A):
				n = length(A)
				for i = \floor{\frac{n}{2}} down to 1:
					MaxHeapify(A, i)
		\end{lstlisting}
		At depth $d$, there are at most $2^d$ nodes\\
		Each node at depth $d$ had height $h-d$\\
		Time complexity $\le \sum_{d=0}^{h-1}2^d(h-d)\le \sum_{i=1}^{h} 2^h-i$, let $i=h-d$\\
		$\le 2^h\sum_{i=1}^{h}\frac{i}{2^i} < 2^h\sum_{i=1}^{\infty}\frac{i}{2^i} \le n\sum_{i=1}^{\infty}\frac{i}{2^i} < 2n = O(n)$	\\\\	
	
	\section{Week 3}
		\subsection{Day One: BST\\}
		Readings: AVL trees on website\\
		Exercises:
		\begin{itemize}
			\item write pseudocode for insert, delete, search
			\item why does deleting the successor fall into case 1 or 2?
			\item 12.1-1, 12.2-1, 12.2-4, 12.3-1, 12.3-5\\
		\end{itemize}
		\textbf{Binary Search Tree Review}\\
			def: binary tree satisfying BST property for any node x\\
			x.key $\ge$ LEFT(x).key, if LEFT exists\\
			x.key $\le$ RIGHT(x).key, if RIGHT exists\\\\
			Problem with "regular" BSTs (fig 3.1.1)\\
			INSERT(4, 5, 6, ..., n) creates an unbalanced BST, so you cannot assume that after inserting values the BST will stay balanced\\\\
			AVL trees\\
			First, we define \textbf{\emph{balance factor}} BF, (fig 3.1.2)\\
			BF(v) = h$_r$ - h$_l$\\
			definition of AVL: BST T where for each $v\in T$
			$$- 1\le BF(v)\le + 1$$
			$$h\le 1.44*log(n+2) \implies h\in\Theta(log n)$$
			Examples of AVL trees:(fig 3.1.3)\\
			\textbf{Benefits}:
			\begin{itemize}
				\item $h\in\Theta(log n)$
				\item can do inserts, deletes, while efficiently maintaining height balance
				\item elegant, relatively simple, works well in practice
			\end{itemize}
			INSERT(T, x): general idea
			\begin{enumerate}
				\item insert x into T as in any BST x is now a leaf
				\item go from x to the root of T, and\\
					- adjust BFs\\
					- rebalance, if necessary
				\item (fig 3.1.4)
			\end{enumerate}
			\textbf{How to rebalance?: rotations\\}
			Single Right Rotation Example (fig 3.1.5)\\
			Double Rotation Example (fig 3.1.5)\\
			Rotation Explanation Example (fig 3.1.7)\\\\
		
		\subsection{Day Two: More on Rebalancing BSTs\\}
		Let \textbf{A} be the \emph{first} node on the path from \textbf{X} to the root of the AVL Tree T that loses its balance, \\ i.e. BF(\textbf{A}) becomes +2 or -2 after \textbf{X} is inserted.\\\\
		Two cases of inserting a node: (fig 3.2.3)\\
		Two subcases of case 1: (fig 3.2.1)\\
		\textbf{Claim: } BF(B) = 0 before insertion of x
		\begin{center}
			why?\\
		\end{center}
		otherwise insertion balances B without changing its height\\ 
		(+/- 1 to 0) or insertion changes BF(B) to +/- 2 ~~(fig 3.2.2)\\\\
		This Rotation:\\
		a) Preserves the BST order (T$_1$, A, T$_2$, B, T$_3$)\\
		b) Rebalances the height to what it ws before the insertion of x (h + 2)
		Note: nodes above subtree are not affected by insertion of x since height is restored\\\\
		Example for insertion that requires a double rotation to balance: (fig 3.2.4)\\\\
		(High-Level) Algorithm for INSERT(T, X):
		\begin{itemize}
			\item insert X as with any BST, X is now a leaf
			\item set BF(X) = 0
			\item go from X to the root, updating BFs for each node, i, encountered
			\begin{itemize}
				\item if X is in the right subtree of increment BF(i), otherwise decrement
				\item if BF(i) = 0, then stop
				\item if BF = +/- 1, i$<$- parent(i), continue
				\item if BF = +2
				\begin{itemize}
					\item let j = rightchild(i)
					\item if BF(j) = +1, single left rotation
					\item if BF(j) = -1, double right-left rotation\\
					Note: update BFs of rotated nodes
				\end{itemize}
				\item if BF(i) = -2 (cases are symmetric)
				\item i=root, stop\\
			\end{itemize}
		\end{itemize}
		\subsection{Lab 3: BST Deletion}
		\textbf{Deletion Operation for BSTs:}
		\begin{itemize}
			\item If $x$ has no children: just delete it
			\item If $x$ has 1 child, delete x and link x's \emph{parent} to x's \emph{child} subtree
			\item If $x$ has 2 children 
			\begin{enumerate}
				\item Find the left most node, $y$, in the right subtree of $x$ 
				\item Replace $x$ with $y$
				\item Delete $y$\\
			\end{enumerate}
		\end{itemize}
		\textbf{Deletion Operation for AVL Trees:} (Balanced BSTs)\\$\|h(left)-h(right)\|\le 1$\\ (fig 3.3.2 \& fig 3.3.4)
		\begin{enumerate}
		\item
		\begin{itemize}
			\item If $x$ has no children: just delete it
			\item If $x$ has 1 child, delete x and link x's \emph{parent} to x's \emph{child} subtree
			\item If $x$ has 2 children
			\begin{enumerate}
				\item: Find the left most node, $y$, in the right subtree of $x$ 
				\item: Replace $x$ with $y$
				\item: Delete $y$
			\end{enumerate}
		\end{itemize}
		\item Rotation\\
	\end{enumerate}
	\textbf{Examples of basic rotations: }(fig 3.3.3)\\\\
\section{Week 4}
	\subsection{Day One: RANK\\}
	Look over sudo code of RANK\\\\
	Finding Rank in Data Structures:
	\begin{itemize}
		\item Heap or Bin heap?
		\begin{itemize}
			\item cannot search efficiently
		\end{itemize}
		\item unmodified AVL tree?
		\begin{itemize}
			\item INSERT(), SEARCH(), DELETE(), $\Theta(log n)$
			\item RANK(k) - inorder traversal until you find k $\Theta(n)$
			\item SELECT(r) - inorder traversal until you find r$^{th}$ element $\Theta(n)$
		\end{itemize}
		\item AVL Tree with added field node.rank for each node
		\begin{itemize}
			\item RANK(k) - return SEARCH(k).rank $\Theta(log n)$
			\item SELECT(r) - ranks are ordered like keys, why?\\
			smaller keys = smaller ranks\\ $\Theta(log n)$
			\item INSERT/DELETE - worst-case: you have to update all ranks in tree (ex: updating minimum) $\Theta(n)$
		\end{itemize}
	\end{itemize}
	$*$Intuition: for each node x, rank(x) = 1 + \# of smaller nodes\\
	So for each node x, store the size of the subtree rooted out x, use size to compute ranks.\\\\
	AVL Tree with nodes containing the value of the size of children (fig 4.1.1)\\
	Rank(x), (fig 4.1.2)\\\\
	RANK Sudo Code: 
	\begin{lstlisting}
	RANK(T,x):
		r = x.left.size + 1
		y = x
		while y != T.root:
			if y == y.parent.right:
				r = r + y.parent.left.size + 1
			y = y.parent
		return r
	\end{lstlisting}
	~\\
	SELECT(r): (fig 4.1.3)\\
	SELECT sudo code:
	\begin{lstlisting}
	SELECT(r):
		if r == N.left.size + 1: return N
		
		if r < N.left.size + 1: recurse on N.left and r
		
		if r > N.left.size + 1: recurse on N.right and r -= N.left.size - 1
	\end{lstlisting}
	Time is $\Theta(log n)$ since we are constant per level \\\\
	INSERT(x):\\
	- size of new node = 1\\
	- increment size of ancestors\\
	DELETE(x):\\
	- same as insert, update size of ancestors\\\\
	WHAT ABOUT ROTATIONS?:\\
	(fig 4.1.4)\\\\
	
	\subsection{Day Two: Hashing}
	\textbf{Problem 1:} read text file keeping track of the number of occurrences of each character (ASCII 0-127)\\
	\textbf{Solution: } Use an array of 128 elements, Direct access table\\\\
	\textbf{Problem 2:} Read data file, keeping track of number of occurrences of each int value (0-2$^{32}-1$)\\
	\textbf{Solution: } Direct Access Table? All operations are $\Theta(1)$ But extremely space inefficient\\
	So in summary: if $\|U\|$ is small and you know it ahead of time use a direct access table, otherwise use a hashtable\\\\
	\textbf{Hash Table:}\\
	- Universe of keys $U$ (set of all possible keys)\\
	- has table, T, array with m positions (m, depends on application)\\
	- has function h: $U -$ \{0, 1, ..., m-2, m-1\}\\\\
	\textbf{HASH-TABLE-SEARCH(T,K):} T[h(k)] \\
	what happens when $\|U\| > $m?\\
	Collision! (pigeonhole principle) ($k_1 != k_2 \bigcap h(k_1) = h(k_2)$)\\\\
	\textbf{Two Issues:}\\
	- handling collisions: (when they happen)\\
	- good hash functions\\\\
	\textbf{Chaining} (probing-chp 11.4-another way to deal with collisions)\\
	- each location of, T, stores a linked list of items that hash to this location\\
	(fig 4.2.1)\\\\
	\textbf{HASH-TABLE-INSERT(T,K)}\\
	insert new node at head of T[h(k)] - $\Theta(1)$\\\\
	\textbf{HASH-TABLE-DELETE(T,K)}\\
	search linked list at T[h(k)] and delete node from linked list, runtime: same as search\\\\
	\textbf{HASH-TABLE-SEARCH(T,K)}\\
	search the linked list at T[h(k)]\\
	Worst-Case for Search?\\
	$\Theta(n)$: all keys hash to same slot\\
	Good news: worst-case rarely is encountered. hashing works well in practice.\\
	\emph{Expected }runtime of SEARCH w/ chaining\\\\
	\textbf{Simple Uniform Hashing Assumption: } assume any key is equally likely to hash into any bucket.\\
	the expected number items per bucket should be the same. (fig 4.2.2)\\
	def$^n$: load factor $\alpha$\\
	$\alpha = $ expected number of items in each bucket, $\alpha = \frac{n}{m}$ under "SUHA"\\\\
	In hashtable in which collisions are resolved by chaining what is the expected runtime of an unsuccessful search? (under SUHA)\\
	$\Theta(\frac{n}{m})$\\
	\textbf{Note: we assume computing hash function is constant}\\\\
	$\Theta(\frac{n}{m}) = \Theta(1)$ If $n\alpha m$\\
	We try to allocate m-size array that is proportional to n
	$$n\alpha m \iff n=am$$
	\textbf{"Good" hash function: }
	\begin{itemize}
		\item approx. satisfies SUHA
		\item independent of where any other key hashed to.\\ 'pt','pts'\\
	\end{itemize}
	Difficult to prove SUHA\\
	- need to know the distribution of the keys\\
	- keys might not be drawn independently\\
	h(k) = k mod m, (be picky with m - m should not be a power of 10 or 2)\\\\
	
	\subsection{Lab 4: Basic Problem Theory}
	\textbf{Sample Space: }set of all possible outcomes\\
	\textbf{Event: }nonempty subset of the sample space\\
	\textbf{Distribution: }Assigns a probability to each outcome s.t. $$\forall O\in S, P_r (O) \le 0 ~~\bigcap~~ \sum_{O\in S} P_r (O) = 1$$
	\textbf{Random Variables: }Mapping of each outcome for real values\\
	\textbf{Expectation of a Random Variable: }(Mean/Average) $$E(x)=\sum_{O\in S} X(O) P_r(O) = \sum_{y\in X} yP_r(X=y)$$
	\textbf{L... of Expectation: } $x_1,...,x_t$ are R.V.s $$E(\sum_{i=1}^{t} x_i = \sum_{i=1}^{t} E(x_i)$$
	$P_r(x\bigcup y) = P_r(x) + P_r(y) - P_r(x\bigcap y)$\\
	$P_r(x\bigcap y) = P_r(x|y)P_r(y)$\\\\
	\textbf{Average Case Running Time: }$t_n(x) =$ Running time of the algorithm on input x\\
	$x_n=$ Set of all inputs of length n\\
	Average Case Running Time: $$\sum_{x\in X_n} t_n(x)P_r(x)$$
	\begin{lstlisting}
	SearchList(H, key):
		while H is not NULL:
			if H.value == key:
				return TRUE
			H = H.next
		return FALSE
	\end{lstlisting}
	Linked List has elements $a_1,...,a_8$\\
	\textbf{Distribution on Key: }with probability $\frac{1}{2}$, key is not in \{$a_1,...,a_8$\}\\
	with probability $\frac{1}{16}$, key = a;, for each\\
	\section{Week 5}
	\subsection{Day One: Bloom Filters}
	\textbf{Bloom Filters:} a "probabilistic/approximate" dictionary. Maintains a summary or fingerprint, $F_S$, of a dynamic set S\\
	\textbf{OPERATIONS:}\\
	BF-INSERT($F_S$,x): $S_i$ = $S_u\{x\}$\\
	BF-SEARCH($F_S$,x): \[ \begin{cases} 
	"no" \implies x\in S\\
	"probably yes" \implies probably x\in S (but perhaps not)\\
	\end{cases}
	\]
	For now assume no delete operation $\implies$ "add-only" dictionary - several applications are like this (i.e actual dictionaries)\\
	\textbf{Trade-Off:} Very space-efficient, and never false negative but possibility of false positives(low prob)\\
	NOTE: The more space we have for BF, the smaller the probability of false positive\\
	All Applications:\\
	- add-only dictionaries\\
	- keeping actual dictionary is too expensive (in terms of space/time to access)\\
	- can tolerate false positives if theyre rare\\
	These are the conditions under which you should consider using Bloom Filters\\\\
	\textbf{How do they work?:}\\
	Allocate an array of m bits BF\[0...m-1\], initially all Os (the laarger the m, the lower the prob of false positives) Define $t\ge 1$ different hash functions $h_1,...,h_t$
	$$h_i: U \Rightarrow \{0,1,..,m-1\}$$
	NOTE: These should be "independent" of each other
	\begin{lstlisting}
		BFINSERT(BF,x):
			for i:=1 to t:
				BF[h_i(x)] := 1
	\end{lstlisting}
	\begin{lstlisting}
		BFSEARCH(BF,x):
			for i:=1 to t:
				if BF[h_i(x)] = 0:
					return "no"
			return "probably yes"
	\end{lstlisting}
	$h_1(x), h_2(x), ..., h_t(x)$ "fingerprint" of x \\
	Both run in $\Theta(t)$, where $t$ is a small constant\\\\
	Why $t$ and not specifically $t=1$?\\
	To minimize the effect of collisions on false positives\\\\
	Two keys that collide under $h_1$ are unlikely to collide under $h_2$ - if we design the hash functions to be independent of each other\\\\
	Parameters to tune minimize false positives:\\
	- m (the bigger the better - but costs space)\\
	- t (not too small, not too large)\\\\
	\textbf{Probability of false positive:}\\
	False positive: when BFSEARCH(x) returns "prob yes" although $x\notin S$\\
	Insert n keys $x_1,x_2,...,x_n$ into BF under $h_1,...,h_t$\\
	Look for a key $x\not= x_1,...,x_n$\\\\
	What is the probability that BF[$h_j(x)$]=1 for all $1\le j\le t$?\\
	Find prob[BF[l]=0 after $x_1,...,x_n$ are inserted], where l, $ 0\le l<m$\\
	= prob[$h_j(x_i)\not= l$, for each key $x_i$ and hash function $h_j$]\\
	$h_j(x_i)\not= l \implies 1 - \frac{1}{m}$ (prob that $h_j(x_i)\not= l$)\\($\frac{1}{m}$ is the prob that $h_j(x_i) = l$ [SUHA])\\
	= ($1-\frac{1}{m})^{nt} \approx e^{-\frac{nt}{m}}$
	
	\subsection{Day Two: QuickSort\\}
	\begin{lstlisting}
	QUICKSORT(A):
		if |A| <= 1: return A
		else:
			select pivot p in A (deterministic or random)
			Partition elements of A into:
				L = {x in A | x <= p}
				G = {x in A | x > p}
			return QUICKSORT(L) + [p] + QUICKSORT(G)
	\end{lstlisting}
	Deterministic and Random pivots are typically similar in overall efficiency\\
	\textbf{WORST-CASE: }
	\begin{itemize}
		\item running time is dominated by Partition
		\item to simplify analysis: consider number of comparisons between elements of A
		\item worst-case: either L or G is always empty. Then C(n) = (n-1) + (n-2) + ... + 1 $\in O(n^2)$
		\item worst-case is a rare occurrence
	\end{itemize}
	\textbf{Expected runtime:}
	\begin{itemize}
		\item X: total number of comparisons performed in all Partition operations
		\item Goal: Compute X
		\item First: rename elements of A as $Z_1, Z_2, ..., Z_n$, where $Z_i$ is the $i^{th}$ smallest element
		\item Define $Z_{ij} = \{Z_i, Z_i+1,...,Z_j\}$
		\item First Observe: each pair of elements in A is compare at most once\\
		$X_{ij} = I\{Z_i$ is compared to $Z_j$ the first time. Then $Z_i$ and $Z_j$ are separated$\}$
		$$\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}x_{ij}$$
		Taking Expectations of both sides:
		$$E(X) = E(\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}x_{ij})$$
		By linearity of Expectation:
		$$ = \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}E(x_{ij})$$
		$ = \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}P_r\{Z_i$ is compared to $Z_j$ when they are seperated$\}$\\\\
		e.g $\{1, 2, 3, 4, 5, 6, $\textbf{\emph{7}}$, 8, 9, 10\}$\\
		7 is the pivot\\
		Partition result is $\{1, 2, 3, 4, 5, 6\}$ and $\{8, 9, 10\}$\\
		$X_{1,10} = 0$ - 7 is compared to all - no 2 elements belonging to different sets will be compared\\
		$X_{7,1} = 1$\\\\
		\item \textbf{Consider: }$Z_{ij} = \{Z_i, Z_{i+1}, ..., Z_j\}$\\
		\begin{itemize}
			\item Once $Z_i < X < Z_j$ is chosen, $Z_i$ and $Z_j$ will never be compared\\
			\item what if $Z_i$ is chosen? $Z_i$ is compared to all elements in $Z_{ij}$
			\item same for $Z_j$
		\end{itemize}
		\item $P_r[Z_i$ is compared to $Z_j$ when they are seperated$]$\\
		$= P_r[Z_i$ or $Z_j$ is the first pivot chosen from $Z_{ij}]$\\
		$= \frac{2}{j-i+1}$ denom is the number of pivots to choose from\\
		$$\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\frac{2}{j-i+1}$$
		Let k = j-i
		$$E(X) = \sum_{i=1}^{n-1}\sum_{j-i=1}^{n-i}\frac{2}{j-i+1} = \sum_{i=1}^{n-1}\sum_{k=1}^{n-i} \frac{2}{k+1} < \sum_{i=1}^{n-1}\sum_{k=1}^{n-i} \frac{2}{k}$$
		$$ \sum_{i=1}^{n-1} O(lg n) = (n-1)O(lgn) = O(nlgn)$$
		$$\sum_{k=1}^{n-i}\frac{1}{k} \le lgn + 1$$
	\end{itemize}
	~\\\\
	\subsection{Lab 4: Augmented Data Structure\\}
	\textbf{Closed Interval:} (fig 5.3.1) an ordered pair of real numbers $[t_1, t_2]$ with $t_1\le t_2$ $$x\in [t_1, t_2]\iff t_1\le x\le t_2$$\\
	\textbf{Goal:} Store a set S of intervals\\
	\textbf{Operations:} \\1. INSERT(S, x),\\ 2.DELETE(S,x):interval, \\3.SEARCH(S,x): Return y s.t. x and y overlap, Return NULL if y doesn't exist\\
	\textbf{Time:} O(log n)\\
	\textbf{Step 1:} AVL Tree\\
	\textbf{Step 2:} Chooose low endpoints as keys\\
	\textbf{Step 3:} augment: M = max(high endpoints in the subtree) for each node\\
	(fig 5.3.2): Updating M is logn\\
	\begin{lstlisting}
	Search(T,x):
		y=root(T)
		while y != Null and (x and y dont overlap):
			if left(y) != NULL and M(left(y)) >= low(x):
				then y=left(y)
			else:
				y=right(y)
		return y
	\end{lstlisting}
	
	\section{Week 6}
	\subsection{Day One: ADT - Disjoint Sets\\}
	\textbf{Objects:} collection of nonempty disjoint dynamic sets $$S= \{s_1, s_2, ..., s_k\}$$
	*Each set is identified by unique representative\\\\
	\textbf{Operations: }MAKE-SET(x): creates new set $S=\{x\}$\\
	UNION(x,y):unites sets containing x and y into a new set\\
	FIND-SET(x): returns a pointer to representative of set containing x\\\\
	MAKE-SET(1)\\
	$S=\{1\}$\\
	MAKE-SET(2)\\
	$S=\{\{1\},\{2\}\}$\\
	MAKE-SET(3)\\
	$S=\{\{1\},\{2\},\{3\}\}$\\
	FIND-SET(2) = 2\\
	UNION(1,2)\\
	$S=\{\{1,2\},\{3\}\}$\\
	FIND-SET(2) = 1\\
	UNION(1,3)\\
	$S=\{\{1,2,3\}\}$\\
	FIND-SET(3) = 1\\\\
	Q: What is the most number of UNIONS we can perform after n MAKE-SETs?\\
	A: n-1\\\\\\
	We want a data structure that minimizes the total cost of performing a sequence $\sigma$\\
	$\sigma$: n-1 UNIONs and $m\ge n$ FIND-SETs ($\implies > n$ MAKE-SETs)\\\\
	\begin{enumerate}
		\item \textbf{Linked List}\\
		$S_1$ (fig 6.1.1)\\
		$S_2$ (fig 6.1.2)\\
		FIND-SET(x): O(n)\\
		UNION(x,y): O(1) (just have the head of the second set be added as the tail of the first)(fig 6.1.3)\\
		Q: What is the cost of $\sigma$?\\
		A: O(n + m*n) where UNIONS: O(n) and FIND-SETs: O(m*n)\\
		\item \textbf{Augmented linked list}\\
		$S_1$ (fig 6.1.4)\\
		$S_2$ (fig 6.1.5)\\
		FIND-SET(x): O(1)\\
		UNION(x,y): O(n) (fig 6.1.6)\\
		\textbf{Q:} What about a sequence of n-1 UNIONs?\\
		MAKE-SET($X_1$), ... , MAKE-SET($x_n$)\\ 
		Number of pointers updated:\\
		UNION($x_2,x_1$): 1\\
		UNION($x_3,x_2$): 2\\
		UNION($x_4,x_3$): 3\\
		UNION($x_n,x_{n-1}$): n-1\\
		UNION(x,y): $\sum_{i=1}^{n-1}i=\Theta(n^2)$\\
		
		\item \textbf{Augmented Linked List with weighted union heuristic}\\
		Suppose each list also contains its length (easily maintained)\\
		Then: we can always append smaller list to bigger list\\
		UNION($s_1,s_2$): Runtime of $\sigma$ is O(m+nlogn)\\
		Analysis of runtime:\\
		*How many times is a single element's representative pointer updated?\\
		Fact: each time x's pointer was updated, x must have started in the smaller set\\
		$\implies$ x's resulting set must have at least double\\
		$1^{st}$ time x's representative was updated, x's resulting set had at least 2 elements\\
		$2^{nd}$ time x's representative was updated, x's resulting set had at least 4 elements\\
		k$^{th}$ time x's representative was updated, x's resulting set had at least $2^k$ elements\\
		k is at most $\floor{log n}$ ~~~~($2^{\floor{log n}} \le n$)\\
		$\implies$ n UNIONs is O(nlogn)\\
		cost of $\sigma$ is O(m+nlogn)\\
		\item \textbf{OPTIMAL: Disjoint Forest}\\
	\end{enumerate}
	
	\subsection{Day Two: Disjoint Forest}
	\textbf{Forest Structure: \\}
	- Each set is an "inteverted tree" where elements only have pointers to parents\\
	- Representative is the root\\
	\textbf{Note:} trees are not necessarily binary\\\\
	UNION($S_1,S_2$) - (fig 6.2.1)\\\\
	MAKE-SET(x): create new tree with root x - O(1)\\
	UNION(i,j): make root of one tree child of another - O(1)\\
	FIND-SET(x): follow pointers to root - O(depth of node x)\\\\
	Worst-Case sequence?:\\
	- n-1 UNIONs can create a tree that is 1 long chain - (fig 6.2.2)\\
	If we call FIND-SET($x_n$) m times, we get $\Theta(n*m)$\\
	How to avoid increasing height of tree?\\ - append smaller tree to larger - (fig 6.2.3)\\
	Heuristics to improve performance?\\
	- UNION by rank:\\
	definition of rank: an upper bound on the height of the tree could be equal to true height, but might bee more because not always efficient to maintain height\\
	MAKE-SET(x): same as before + set rank(x) = 0\\
	UNION(i, j): the node with higher rank is the new root and it's rank is the same\\
	If both i and j have the same rank, pick first one as root and increase its rank by 1\\
	\textbf{Path Compression:\\}
	During FIND-SET(B): (fig 6.2.4)\\
	Total cost of Executing (n-1 UNIONS, $m\ge n$ FINDs) is:\\
	- $\theta(mlog(*n)$\\
	- $O(m\alpha^{-1}(m,n))$\\
	- $\Omega(m\alpha^{-1}(m,n))$\\\\
	\subsection{Lab 5: Disjoint Sets}
	Linked List Implementation: (fig 6.3.1)\\
	2 Parameters:\\
	- n: number of MAKESETS (at most n-1 UNIONS)\\
	- m: number of FINDSETS\\\\
	\textbf{Theorem: }Using the linked list representation of a disjoint set and the weighted union heaurisitc, a sequence if n-1 unions and m findsets takes O(m+logn) time\\
	\textbf{UNION:}\\
	 1$^{st}$ update $\implies$ $2\le$ elements\\
	2$^{nd}$ update $\implies$ $4\le$ elements\\
	k$^{th}$ update $\implies$ at least $2^k$ elements\\
	Time: O(nlogn)\\
	\textbf{FINDSET:} Time: O(1)\\\\
	Q: Consider the following operation of disjoint sets:\\
	PRINT(x): Prints every element in $S_x$ (the set containing x) show how to augment the tree data structure to implement PRINT(x) in O($|S_x|$) time without affecting the runtime of the outer operations\\
	A: keep track of the tail, when calling union change the pointer to the tail
	
	\section{Week 7}
	\subsection{Day One: Amortized Analysis and beginning of Dynamic Tables}
	\textbf{Motivation:} looking at worst-case time complexity per operation can be too pessimistic\\
	\textbf{Definition:} Average time required to perform a sequence of operations on a data structure over all operations performed\\\\
	Example of where worst-case is overly pessimistic: fig 7.1.1\\\\
	\textbf{Three Methods:}\begin{enumerate}
		\item Aggregate
		\item Accounting
		\item Potential (Not covered in this course, but is in textbook)
	\end{enumerate}
	\textbf{Aggregate Analysis:\\}
	A sequence of n operations takes worst-case time T(n) in total\\
	$\approx$ amortized cost is $\frac{T(n)}{n}$\\\\
	\textbf{Example:} incrementing a binary counter\\
	The counter: k-bit binary counter that starts counting from 0\\
	cost = number of bit flips\\
	The array: fig 7.1.2\\
	k=4 example: fig 7.1.3\\
	worst-case: all bits are flipped - O(k)\\
	worst-case for a sequence of k is O($k^2$)\\\\
	The Aggregate Analysis of fig 7.1.3\\
	- bit at i = 0 flips everytime $\Rightarrow$ n times\\
	- bit at i = 1 flips every 2$^{nd}$ time $\Rightarrow \floor{\frac{n}{2}}$\\
	- bit at i = j flips every $2^j$ times $\Rightarrow \floor{\frac{n}{2^j}}$\\\\
	In general, A[i] flips $\floor{\frac{n}{2^i}}$ times\\
	Total number of flips in sequence: $$\sum_{i=0}^{k-1}\floor{\frac{n}{2^i}} < n\sum_{i=0}^{\infty}\frac{1}{2^i} = 2n < O(n)$$
	T(n) = O(n) $\Rightarrow \frac{T(n)}{n} = O(1)$ (Amortized cost of an operation)\\\\
	\textbf{Accounting Method\\}
	IDEA: impose an extra charge on inexpensive operations and use it to "pay for" expensive operations later.\\\\
	Accounting method for binary counter:\\
	What should we charge for an increment operation?\\
	\$2/increment (shows we have O(1) amortized)\\
	- \$1 to set a bit to 1\\
	- leave \$1 on the bit (to be used later when we set back to 0)\\\\
	Outline of Proof:\\
	(we always have money for operations)\\
	1. Every 1 bit has a \$ on it (simple proof by induction)\\
	2. At most one 0 bit gets flipped to 1\\\\
	\textbf{Dynamic Tables\\}
	In C:\\
	var a[20]; $\leftarrow$ contiguous space in memory\\
	In Python:\\
	var a = [] (python copies the array, when the space is filled, into a larger array)\\
	
	\subsection{Day Two: Dynamic Tables}
	
	declaring an array in C: type arrayName[array Size];\\
	vs Python: l = [] - dynamic array\\\\
	- dynamically expanding and contracting array\\ 
	- also: \begin{itemize}
			\item keep insertion and deletion - O(1) amortized\\
			\item guarantee unused space never exceeds constant fraction of total space\\
			\item we define a load factor $\alpha$(T)\\
			$$\alpha(T) = \frac{\#~of~items~stored~in~T}{size~of~T}$$
			$$\alpha(T) \ge c$$
	\end{itemize}
	\textbf{Common heuristic:} allocate a new table with twice as many slots, and copy all elements over\\\\
	\textbf{Consider} just insertions:\\
	\[ \begin{cases} 
	1 & if~there~is~room \\
	i & if ~there~isn't\\ 
	\end{cases}
	\]
	note: $i - 1$ is a power of 2\\
	$$\sum_{i=1}^{n}c_i < n + \sum_{j=0}^{\floor{logn}}2^j = 2^{\floor{logn} + 1}$$
	Recall:
	$$\sum_{i=0}^{k}2^i = 2^{k+1} - 1$$
	$< n + 2n = 3n$ - amortized per operation $\Rightarrow \frac{3n}{n} \in O(1)$\\\\
	
	\textbf{Accounting Method\\}
	\$3/item
	\begin{itemize}
		\item \$1 insert itself into array
		\item \$1 move itself when table expands
		\item \$1 move over another item that has already been moved\\
	\end{itemize}
	Suppose table has just been expanded (fig 7.2.1)\\
	\textbf{Note:} there is no credit in the table\\
	\begin{itemize}
		\item For the next expansion to occur $\frac{m}{2}$ elements will be inserted
		\item There will be \$2$*\frac{m}{2} = \$m$ in credit\\
	\end{itemize}
	Table expansion and contraction:\\
	want to bound load factor above and below by positive constant\\\\
	\textbf{IDEA (flawed):} halve when less than half full and double when full (fig 7.2.2)\\\\
	\textbf{IDEA:} double when table is full and halve when $<\frac{1}{4}$ full\\
	\textbf{Accounting Method: (fig 7.2.3)}
	\begin{itemize}
		\item insertion: same
		\item deletion: \$2 per operation
		\item \$1 to perform deletion
		\item \$1 stored in empty set
	\end{itemize}
	
\end{document}